{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prerequisits\n",
    "\n",
    "1. Make sure to install [SeisMIC](https://github.com/PeterMakus/SeisMIC) version 0.5.14 or later. Currently, this version is only available over the GitHub `dev` branch. Follow the instructions provided [here](https://petermakus.github.io/SeisMIC/modules/get_started.html#via-github) to install SeisMIC's developer version.\n",
    "2. Compute Cross-Correlations between (a) the stations that you suspect to have a clock shift and stations that are operating normally. Follow the instructions in [SeisMIC's tutorial](https://petermakus.github.io/SeisMIC/modules/tutorials.html) to do so. For the example at MSH, we will want to have data between approximately May 2013 and February 2014 and data from the stations *UW.EDM*, *UW.FL2*, *UW.HSR*, *UW.SHW*, *UW.JUN*, *UW.SOS*, *UW.YEL* (the stations with  a clock shift) and some stable stations, I used: *CC.VALT*, *PB.B202*, and *PB.B204*. Note that you will not have to compute the cross-correlations between the UW stations as they are all digitised over a common clock (i.e., no clock shift will be visible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Compute clock shift for each station pair.\n",
    "\n",
    "You will have to adapt the parameters. The code is still fairly exemplary. Read the source code, there are some options that you might want to change. I gave some suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some necessary modules\n",
    "import os\n",
    "import glob\n",
    "import fnmatch\n",
    "from copy import deepcopy\n",
    "\n",
    "from obspy import UTCDateTime\n",
    "import numpy as np\n",
    "\n",
    "from seismic.db.corr_hdf5 import CorrelationDataBase as CorrDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions for below\n",
    "\n",
    "def check_data_availability(infile, time_to_check: UTCDateTime):\n",
    "    \"\"\"\n",
    "    Check if the data is available inside the h5 file. Also check what\n",
    "    channel combinations are there\n",
    "    \"\"\"\n",
    "    netcode, statcode = os.path.basename(infile).split('.')[:2]\n",
    "    with CorrDB(infile, mode='r') as cdb:\n",
    "        chans = cdb.get_available_channels(\n",
    "            'subdivision', netcode, statcode)\n",
    "        av_starts = cdb.get_available_starttimes(\n",
    "            netcode, statcode, 'subdivision', chans[0]\n",
    "        )\n",
    "        av_starts = np.array([UTCDateTime(x) for x in av_starts[chans[0]]])\n",
    "        if min(abs(av_starts - time_to_check)) > 86400:\n",
    "            print(f'skipping {infile} because of missing data')\n",
    "            return []\n",
    "    return chans\n",
    "\n",
    "\n",
    "def compute_clock_drift_for_cha(infile, chacode, starttime, endtime, tw):\n",
    "    \"\"\"\n",
    "    Compute the clock drift for a single channel\n",
    "    \"\"\"\n",
    "    netcode, statcode = os.path.basename(infile).split('.')[:2]\n",
    "    with CorrDB(infile, mode='r') as cdb:\n",
    "        try:\n",
    "            cst = cdb.get_data(\n",
    "                netcode, statcode, chacode, 'subdivision')\n",
    "            if cst.count() == 0:\n",
    "                raise IndexError\n",
    "        except (KeyError, IndexError):\n",
    "            print(\n",
    "                f'For {netcode}.{statcode}.{chacode} no data is '\n",
    "                f'available between {starttime} and {endtime}'\n",
    "            )\n",
    "            return False\n",
    "    try:\n",
    "        cb = cst.create_corr_bulk(\n",
    "            inplace=True, times=[starttime, endtime], channel=chacode)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "    # Option 1: Use the first 90 days as reference. To adjust 0 properly\n",
    "    # And resample the\n",
    "    # Correlation Functions evenly. This will help you to accurately locate\n",
    "    # the drop\n",
    "    starttimes_new = np.array([starttime + dt for dt in np.arange(\n",
    "        (endtime-starttime)//3600)*3600]) \n",
    "    endtimes_new = starttimes_new + 3600\n",
    "    cb = cb.resample(starttimes_new, endtimes_new)\n",
    "    ref_trc = cb[:90*24].extract_trace()\n",
    "    cb.smooth(24*10)\n",
    "    \n",
    "    # option 2: Resample the Correlation Functions to only two functions.\n",
    "    # One before and one after the drop occurs. Then compare the two\n",
    "    # If you use this one, you need to comment out the above lines\n",
    "\n",
    "    # cb = cb.resample(\n",
    "    #     np.array([starttime, UTCDateTime(2013, 10, 3)]),\n",
    "    #     np.array([UTCDateTime(2013, 10, 25), endtime]))\n",
    "    # ref_trc=cb[0]\n",
    "    \n",
    "    # You can adjust the parameters. For instance shift range=1 checks\n",
    "    # for shifts of maxmimum 1s in either direction\n",
    "    dt = cb.measure_shift(\n",
    "        shift_range=1, shift_steps=201, return_sim_mat=True,\n",
    "        tw=tw, ref_trc=ref_trc)\n",
    "    return dt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## Define Parameters. You will have to change things here\n",
    "\n",
    "\n",
    "# Time window to invert the shifts for\n",
    "# We can use several time windows and jointly invert to raise the precision /\n",
    "# to avoid velocity changes being mistaken as clock shifts\n",
    "# Each time window should encompass several periods of your dominant frequency\n",
    "tw = [[tws, twe] for tws, twe in zip(np.arange(9)*5 + 5, np.arange(9)*5 + 20)]\n",
    "\n",
    "starttime = UTCDateTime(2013, 5, 1)\n",
    "endtime = UTCDateTime(2014, 2, 1)\n",
    "\n",
    "droptime = UTCDateTime(2013, 9, 3)\n",
    "\n",
    "infolder = '/This/is/where/the/correlation/files/are'\n",
    "\n",
    "outfolder = '/where/the/output/will/be/saved'\n",
    "\n",
    "## The actual code. This could be parallised using mpi4py\n",
    "\n",
    "os.makedirs(outfolder, exist_ok=True)\n",
    "\n",
    "for infile in glob.glob(os.path.join(infolder, '*.h5')):\n",
    "    netcode, statcode = os.path.basename(infile).split('.')[:2]\n",
    "    chans = check_data_availability(infile, droptime)\n",
    "    for chacode in chans:\n",
    "        print(\n",
    "            f'working on {infile} for {netcode}.{statcode}.{chacode}')\n",
    "        dt = compute_clock_drift_for_cha(\n",
    "            infile, chacode, starttime, endtime, tw)\n",
    "        dt.save(os.path.join(outfolder, f'DT-{dt.stats.id}.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stack and plot your shift\n",
    "In this very particular case, we are fortunate enough that we have a few stations that exhibit exactly the same shift, so we can stack the results to reduce errors. the code below shows you how to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seismic.monitor.dv import read_dv\n",
    "from seismic.monitor.monitor import average_components\n",
    "\n",
    "# averaging is pretty easy\n",
    "\n",
    "# read all the shift files\n",
    "shifts = read_dv(outfolder, 'DT-*.npz')\n",
    "\n",
    "# average them. Under the hood, this stacks the resulting matrix from the\n",
    "# grid search and computes the new maximum\n",
    "avg_shifts = average_components(shifts, save_scatter=False)\n",
    "\n",
    "\n",
    "# ## Plot the results\n",
    "avg_shifts.plot()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
